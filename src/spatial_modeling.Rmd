---
title: "spatial_modeling"
author: "Erfan Kefayat"
date: "2025-02-11"
output:
  slidy_presentation: default
  ioslides_presentation: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(scipen = 999)
```

## R Markdown

This is an R Markdown presentation. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

```{r libraries,include=FALSE, echo = FALSE}
# ================================
# Install & Load Required Packages
# ================================

# List of packages with short descriptions
required_packages <- c(
  # Spatial data handling & mapping
  "sp",          # Base spatial data classes
  "spdep",       # Spatial dependence and autocorrelation
  "sf",          # Simple features for spatial vector data
  "tmap",        # Thematic maps
  "leaflet",     # Interactive maps
  "spgwr",       # Geographically Weighted Regression
  "gwzinbr",     # GWR for Zero-Inflated Negative Binomial Regression
  "mgwnbr",      # Mixed Geographically Weighted Negative Binomial Regression
  "GWmodel",     # Advanced GWR and spatial stats
  
  # Spatial modeling & Bayesian inference
  "spatialreg",  # Spatial regression models
  "spaMM",       # Spatial GLMMs
  "glmmfields",  # GLMMs for spatial and spatiotemporal data
  "CARBayes",    # Conditional Autoregressive Bayesian models
  "bigDM",       # Scalable spatial models
  "nimble",      # Flexible Bayesian modeling
  "geostan",     # Bayesian spatial analysis
  "R2WinBUGS",   # Interface to WinBUGS
  "coda",        # MCMC diagnostics
  "aod",         # Analysis of Overdispersed Data
  
  # Network, distance & movement analysis
  "igraph",      # Network analysis
  "gdistance",   # Geographic distances and least-cost paths
  "adehabitatHR",# Home range estimation (wildlife movement)
  
  # Regression & statistical modeling
  "dplyr",       # Data manipulation
  "tidyr",       # Data tidying
  "ggplot2",     # Visualization
  "lmtest",      # Diagnostic tests for linear models
  "performance", # Model performance metrics
  "car",         # Companion to Applied Regression
  "pscl",        # Political science computation library
  "rsq",         # R-squared calculations
  "psych",       # Psychometric analysis
  "MASS",        # Modern Applied Statistics
  "seqHMM",      # Hidden Markov Models for sequences
  
  # Miscellaneous
  "zoo",         # Time series
  "RcmdrMisc",   # Misc functions from R Commander
  "grid",        # Base R graphics layout
  "gridExtra",   # Arrange multiple grid-based plots
  "here",
  "reticulate",
  "DHARMa"
  
)

# Install any packages that are not yet installed
installed_packages <- rownames(installed.packages())
for (pkg in required_packages) {
  if (!(pkg %in% installed_packages)) {
    install.packages(pkg)
  }
}

# Load all required libraries
invisible(lapply(required_packages, library, character.only = TRUE))


source_python("run_analysis.py")
```

#Model pre-processing

```{r processing,include=FALSE}

# 0. Importing crime column
crime.col <- crime_col                                 

# 1. Read data
original.data <- st_read(here("data/final_dataset/final_components.shp"))
data_additional <- st_read(here("data/neighborhoods/neighborhoods.shp")) |>
  mutate(area = as.numeric(st_area(geometry))) |>
  st_drop_geometry() |>
  select(id, all_of(crime.col), area)

# 2. Join attributes
original.data <- original.data |>
  left_join(data_additional, by = "id")

# 3. Coordinates & neighbors
coords <- st_coordinates(st_centroid(original.data))
original.data$X <- coords[,1]
original.data$Y <- coords[,2]

nb <- poly2nb(original.data, queen = TRUE)
lw <- nb2listw(nb, style = "W")

# 4. Log-transform network PCs
original.data <- original.data |>
  mutate(
    across(matches("^PC[0-9]+_net$"), ~ log(.x + 1), .names = "{.col}_log")   #######################  WARNINg: log with +1 may not work here  ##########################
  ) |>
  select(-matches("^PC[0-9]+_net$"))

# 5. Spatial lag of crime
original.data$spatial_lag <- lag.listw(lw, original.data[[crime.col]])

# 6. Model formulas
independent_vars <- c(
  grep("_log$", names(original.data), value = TRUE),
  grep("^PC[0-9]+_ctrl$", names(original.data), value = TRUE)
)

formula_string <- paste(crime.col, "~", paste(independent_vars, collapse = " + "))

base.model <- as.formula(paste0(formula_string, " + offset(log(area))"))
base.model.no.offset <- as.formula(formula_string)

```

# Crime Count distribution type

```{r overdispersion test, echo = TRUE}

var <- original.data[[crime.col]]
m <- mean(var); v <- var(var)
cat("Overdispersion ratio:", v/m, "\n")

if (abs(m - v) < 0.05*m) {
  print("Mean ≈ variance → Poisson likely.")
} else {
  print("Mean ≠ variance → Poisson unlikely.")
}

if (v/m > 1) {
  print("Overdispersion detected → Negative Binomial appropriate.")
} else {
  print("No overdispersion → NB may not be appropriate.")
}

```

# GLM NB-based modeling and model diagnostics

```{r GLM NB-based, echo = TRUE}
nb.glm <- glm.nb(base.model, data = original.data); summary(nb.glm)# NB GLM
print(paste("R²:", rsq(nb.glm, adj = TRUE)))                       # R²
check_overdispersion(nb.glm)                                       # Overdispersion

# Diagnostics
res.nb <- residuals(nb.glm, type = "pearson")
print(moran.test(original.data[[crime.col]], lw))                  # Moran on dependent
print(lm.morantest(nb.glm, listw = lw))                            # Moran on residuals
bptest(nb.glm)                                                     # BP heteroscedasticity
print(wald.test(b = coef(nb.glm), Sigma = vcov(nb.glm),
                Terms = 2:length(coef(nb.glm))))                   # Wald test
print(lm.RStests(nb.glm, lw, test = "all"))                        # LM spatial dependence tests
```










#CAR NB-based modeling in a Bayesian environment

```{r CAR NB-based, echo = TRUE}

# 1. Create a UID for INLA
original.data$UID <- 1:nrow(original.data)

# 2. Build adjacency graph in INLA format
nb <- poly2nb(original.data)
nb_inla <- nb2INLA("adjacency.adj", nb)   # Creates adjacency.adj file

# 3. Negative binomial CAR formula
formula.car.nb <- propcrimct ~
  PC1_net_log + PC2_net_log + PC3_net_log + PC4_net_log +
  PC1_ctrl + PC2_ctrl +
  f(UID,
    model = "besag",
    graph = "adjacency.adj",
    scale.model = TRUE,
    hyper = list(
      prec = list(prior = "loggamma", param = c(1, 0.01))
    )
  )

# 4. Fit INLA NB model
m.car.nb <- inla(
  formula.car.nb,
  family = "nbinomial",
  data = original.data,
  offset = log(original.data$area),
  control.predictor = list(compute = TRUE),
  control.compute = list(dic = TRUE, waic = TRUE),
  verbose = TRUE
)

summary(m.car.nb)
```

#CAR NB-based model analysis

```{r CAR NB-based analysis , echo = TRUE}

# -----------------------------------------------
# 0. Select the variable you want to analyze
# -----------------------------------------------

target_var <- "average.be_log"   # CHANGE HERE if analyzing another variable

# Check if variable exists in INLA output
if (!target_var %in% names(m0.besag$marginals.fixed)) {
    stop(paste0("Variable '", target_var, "' not found in m0.besag$marginals.fixed."))
}

# Extract marginal distribution for the selected variable
marginal <- m0.besag$marginals.fixed[[target_var]]

# Ensure it is a proper dataframe
marg_df <- as.data.frame(marginal)
colnames(marg_df) <- c("x", "density")

# -----------------------------------------------
# 1. Posterior summaries
# -----------------------------------------------

# Probability(beta < 0)
prob_negative <- inla.pmarginal(0, marginal)

# Probability(beta > 0)
prob_positive <- 1 - prob_negative

# Posterior mean
mean_beta <- inla.emarginal(function(x) x, marginal)

# Posterior SD
var_beta <- inla.emarginal(function(x) x^2, marginal) - mean_beta^2
sd_beta <- sqrt(var_beta)

# -----------------------------------------------
# 2. Data for shaded regions
# -----------------------------------------------
df_negative <- subset(marg_df, x < 0)
df_positive <- subset(marg_df, x >= 0)

# -----------------------------------------------
# 3. Percentiles
# -----------------------------------------------

percentiles <- seq(0.05, 0.95, by = 0.05)
percentile_values <- sapply(percentiles, function(p)
    inla.qmarginal(p, marginal)
)

percentile_table <- data.frame(
    Percentile = paste0(percentiles * 100, "%"),
    Value = round(percentile_values, 4)
)

# -----------------------------------------------
# 4. Plot
# -----------------------------------------------

posterior_plot <- ggplot() +
  geom_area(data = df_negative, aes(x = x, y = density),
            fill = "red", alpha = 0.4) +
  geom_area(data = df_positive, aes(x = x, y = density),
            fill = "blue", alpha = 0.4) +
  geom_line(data = marg_df, aes(x = x, y = density),
            size = 1.2, color = "black") +
  geom_vline(xintercept = 0, linetype = "dashed",
             color = "darkred", size = 0.8) +
  geom_vline(xintercept = mean_beta, linetype = "solid",
             color = "blue", size = 1.2) +
  geom_vline(xintercept = mean_beta + sd_beta, linetype = "dotted",
             color = "darkblue", size = 1) +
  geom_vline(xintercept = mean_beta - sd_beta, linetype = "dotted",
             color = "darkblue", size = 1) +

  annotate("text",
           x = max(marg_df$x) - 0.1,
           y = max(marg_df$density) * 1.05,
           label = paste0("Negative Probability: ", round(prob_negative, 3)),
           color = "red", size = 4, hjust = 1, fontface = "italic") +

  annotate("text",
           x = max(marg_df$x) - 0.1,
           y = max(marg_df$density) * 0.95,
           label = paste0("Positive Probability: ", round(prob_positive, 3)),
           color = "blue", size = 4, hjust = 1, fontface = "italic") +

  annotate("text",
           x = mean_beta,
           y = max(marg_df$density) * 0.85,
           label = "Mean",
           color = "blue", angle = 90, vjust = -0.4, size = 4) +
  annotate("text",
           x = mean_beta + sd_beta,
           y = max(marg_df$density) * 0.75,
           label = "+1 SD",
           color = "darkblue", angle = 90, vjust = -0.4, size = 4) +
  annotate("text",
           x = mean_beta - sd_beta,
           y = max(marg_df$density) * 0.75,
           label = "−1 SD",
           color = "darkblue", angle = 90, vjust = -0.4, size = 4) +

  labs(
    x = paste0("Effect Size: ", target_var),
    y = "Posterior Density"
  ) +
  theme_minimal(base_size = 12)

```

#SAR NB-based modeling in a Bayesian environment

```{r SAR NB-based, echo = TRUE}

# -----------------------------
# 0. Rename for consistency
# -----------------------------
crime_var <- crime.col      # = "propcrimct"
W_listw   <- lw                # spatial weights object
area_log  <- log(original.data$area)   # earlier you used offset(log(area))

# Ensure UID exists
if (!"UID" %in% names(original.data)) {
  original.data$UID <- seq_len(nrow(original.data))
}

# -----------------------------
# 1. Sparse W matrix for INLA
# -----------------------------
W_sparse <- as(as_dgRMatrix_listw(W_listw), "CsparseMatrix")

# -----------------------------
# 2. Formula (must match your variable names)
# -----------------------------
f1 <- as.formula(
  paste(
    crime_var,
    "~",
    paste(
      c(
        "average.be_log",
        "intersecti_log",
        "average.cl_log",
        "average.sh_log",
        paste0("Factor_", 1:9)
      ),
      collapse = " + "
    )
  )
)

mmatrix <- model.matrix(f1, original.data)

# -----------------------------
# 3. Eigenvalues for SLM bounds
# -----------------------------
e_vals <- eigenw(W_listw)
e_real <- Re(e_vals[abs(Im(e_vals)) < 1e-6])

rho.max <- 1 / max(e_real)
rho.min <- 1 / min(e_real)

# -----------------------------
# 4. Priors for β coefficients
# -----------------------------
Q.beta <- Diagonal(n = ncol(mmatrix), x = 1)

# Priors for precision + rho
hyper <- list(
  prec = list(prior = "loggamma", param = c(0.1, 0.1)),
  rho  = list(initial = 0, prior = "logitbeta", param = c(1, 1))
)

# -----------------------------
# 5. SLM latent field
# -----------------------------
formula.spatial_lag <- f1 +
  f(
    UID,
    model = "slm",
    args.slm = list(
      rho.min = rho.min,
      rho.max = rho.max,
      W       = W_sparse,
      X       = mmatrix,
      Q.beta  = Q.beta
    ),
    hyper = hyper
  )

# -----------------------------
# 6. Fit INLA SLM Negative Binomial
# -----------------------------
m.spatial_lag <- inla(
  formula.spatial_lag,
  family = "nbinomial",
  data   = original.data,
  offset = area_log,
  control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE),
  control.predictor = list(compute = TRUE),
  verbose = inla.getOption("verbose")
)

summary(m.spatial_lag)

# -----------------------------
# 7. Extract the true-scale rho
# -----------------------------
rho_index <- grep("Rho", names(m.spatial_lag$marginals.hyperpar), ignore.case = TRUE)

rho_marginal_rescaled <- inla.tmarginal(
  function(x) rho.min + x * (rho.max - rho.min),
  m.spatial_lag$marginals.hyperpar[[rho_index]]
)

inla.zmarginal(rho_marginal_rescaled)

```



```{r SAR NB-based model analysis_new, echo = TRUE}

# -------------------------------
# 0. Select variable to analyze
# -------------------------------
target_var <- "average.be_log"

# Check if the variable exists in INLA output
if (!target_var %in% names(m0.spatial_lag$marginals.fixed)) {
    stop(paste0("Variable '", target_var, "' not found in m0.spatial_lag$marginals.fixed."))
}

# Extract the marginal
marginal <- m0.spatial_lag$marginals.fixed[[target_var]]

# Convert marginal to dataframe (SAFE)
marg_df <- as.data.frame(marginal)
colnames(marg_df) <- c("x", "density")

# -------------------------------
# 1. Posterior summaries
# -------------------------------

# P(beta < 0)
prob_negative <- inla.pmarginal(0, marginal)

# P(beta ≥ 0)
prob_positive <- 1 - prob_negative

# Posterior mean
mean_beta <- inla.emarginal(function(x) x, marginal)

# Posterior sd
var_beta <- inla.emarginal(function(x) x^2, marginal) - mean_beta^2
sd_beta <- sqrt(var_beta)

# -------------------------------
# 2. Shaded regions
# -------------------------------
df_negative <- dplyr::filter(marg_df, x < 0)
df_positive <- dplyr::filter(marg_df, x >= 0)

# -------------------------------
# 3. Percentiles
# -------------------------------
percentiles <- seq(0.05, 0.95, by = 0.05)
percentile_values <- sapply(percentiles, function(p) inla.qmarginal(p, marginal))

percentile_table <- data.frame(
  Percentile = paste0(percentiles * 100, "%"),
  Value = round(percentile_values, 4)
)

# -------------------------------
# 4. Plot posterior density
# -------------------------------
posterior_plot <- ggplot() +
  geom_area(data = df_negative, aes(x = x, y = density), fill = "red", alpha = 0.4) +
  geom_area(data = df_positive, aes(x = x, y = density), fill = "blue", alpha = 0.4) +
  geom_line(data = marg_df, aes(x = x, y = density), size = 1.2, color = "black") +
  geom_vline(xintercept = 0,        linetype = "dashed", color = "darkred", size = 0.8) +
  geom_vline(xintercept = mean_beta,                color = "blue",     size = 1.2) +
  geom_vline(xintercept = mean_beta + sd_beta,      linetype = "dotted", color = "darkblue") +
  geom_vline(xintercept = mean_beta - sd_beta,      linetype = "dotted", color = "darkblue") +

  annotate("text",
           x = max(marg_df$x) - 0.1,
           y = max(marg_df$density) * 1.05,
           label = paste0("Negative Probability: ", round(prob_negative, 3)),
           color = "red", size = 4, hjust = 1, fontface = "italic") +

  annotate("text",
           x = max(marg_df$x) - 0.1,
           y = max(marg_df$density) * 0.95,
           label = paste0("Positive Probability: ", round(prob_positive, 3)),
           color = "blue", size = 4, hjust = 1, fontface = "italic") +

  annotate("text", x = mean_beta,
           y = max(marg_df$density) * 0.85,
           label = "Mean", color = "blue", angle = 90, vjust = -0.4, size = 4) +
  annotate("text", x = mean_beta + sd_beta,
           y = max(marg_df$density) * 0.75,
           label = "+1 SD", color = "darkblue", angle = 90, vjust = -0.4, size = 4) +
  annotate("text", x = mean_beta - sd_beta,
           y = max(marg_df$density) * 0.75,
           label = "−1 SD", color = "darkblue", angle = 90, vjust = -0.4, size = 4) +

  labs(
    x = paste0("Effect Size: ", target_var),
    y = "Posterior Density"
  ) +
  theme_minimal(base_size = 12)

```




























# GWR NB-based modeling and model diagnostics

```{r GWR NB-based , echo = TRUE}


#theta is calculated in the glm.nb model above (2.702)


#Since there is no approach to find local p-values of coefficients, we need to standardize the formula and input it to the model, so the coefficients are comparable.

# Standardizing the variables in original.data
original.data_std <- original.data  # Copy the original data

# Apply z-score standardization (excluding the offset term)
original.data_std$Factor_1 <- scale(original.data_std$Factor_1)
original.data_std$Factor_2 <- scale(original.data_std$Factor_2)
original.data_std$Factor_3 <- scale(original.data_std$Factor_3)
original.data_std$Factor_4 <- scale(original.data_std$Factor_4)
original.data_std$Factor_1 <- scale(original.data_std$Factor_1)
original.data_std$Factor_5 <- scale(original.data_std$Factor_5)
original.data_std$Factor_6 <- scale(original.data_std$Factor_6)
original.data_std$Factor_7 <- scale(original.data_std$Factor_7)
original.data_std$Factor_8 <- scale(original.data_std$Factor_8)
original.data_std$Factor_9 <- scale(original.data_std$Factor_9)
original.data_std$average.be_log <- scale(original.data_std$average.be_log)
original.data_std$intersecti_log <- scale(original.data_std$intersecti_log)
original.data_std$average.cl_log <- scale(original.data_std$average.cl_log)
original.data_std$average.sh_log <- scale(original.data_std$average.sh_log)
original.data_std$crime.count <- scale(original.data_std$crime.count)

View(original.data_std)



# #Select bandwidth using cross-validation
# bandwidth <- ggwr.sel(formula, data = original.data_std,
#                       #adapt = TRUE,
#                       coords = coords_original.data, family = negative.binomial(2.702), verbose = TRUE)

# # Perform the Geographically Weighted Poisson Regression
# gwr_model <- ggwr(formula,
#                   data = original.data_std,
#                   coords = coords_original.data,
#                   bandwidth = 33776.24, #bandwidth selected in the formula above.
#                   family = negative.binomial(2.702),
#                   type = "response"
#                   )
# 
# 
# 
# # View the results
# print(gwr_model)
# summary(gwr_model$SDF@data)
# View(gwr_model$SDF@data)


```

# AIC and Log-liklihood of the GWR NB-based model

```{r GWR NB-based , echo = TRUE}


raw_residuals <- gwr_model$SDF$response_resids

# Calculate the variance of the residuals
sigma_squared <- var(raw_residuals)

# Calculate the log-likelihood
log_likelihood <- -0.5 * (length(raw_residuals) * log(2 * pi) + length(raw_residuals) * log(sigma_squared) + sum((raw_residuals)^2) / sigma_squared)

#Number of parameters:

k <- 15 #including intercept and Dispersion parameter (θ) for Negative Binomial

# Calculate AIC
aic <- -2 * log_likelihood + 2 * (k)

# Print AIC
print(log_likelihood)
print(aic)

```


# GWR NB-based model's local stats

```{r GWR NB-based , echo = TRUE}

original.data_temp <- as(original.data, "Spatial")

gwr_model_local <- gw.cov(original.data_temp,vars = independent_vars,bw = 33776.24, cor = FALSE)


# Add p-values and z-scores to the SDF data
predictor_names <- c("average.be_log", "intersecti_log", "average.cl_log", "average.sh_log",
                     "Factor_1", "Factor_2", "Factor_3", "Factor_4",
                     "Factor_5", "Factor_6", "Factor_7", "Factor_8", "Factor_9")


for (i in 1:length(predictor_names)) {

  mean.predictor <- gwr_model_local$SDF@data[, paste0("mean.", predictor_names[i])]
  se <- gwr_model_local$SDF@data[, paste0("sem.", predictor_names[i])]

  z_value <- mean.predictor / se   # Compute z-scores

  gwr_model_local$SDF@data[, paste0("z_value.", predictor_names[i])] <- z_value

  #df <- nrow(original.data) - length(street_measures) - 1  
  df <- 316.9343 #found in arcgis
  p_values <- round(2 * (1 - pt(abs(z_value),df)), 6)


  gwr_model_local$SDF@data[, paste0("p_value.", predictor_names[i])] <- p_values
}

gwr_model_local$SDF@data <- gwr_model_local$SDF@data %>% 
  dplyr::select(-starts_with("cov."))


original.data_final <- cbind(original.data, gwr_model_local$SDF@data)

# View the updated SDF data with p-values and z-values
View(original.data_final)

```


```{r GWR NB-based , echo = TRUE}

View(gwr_model$SDF@data)


# Initialize an empty data frame to store results
final_results_df <- data.frame()

# Loop through each variable in street_measures
for (var in street_measures) {
  
  local_coef <- gwr_model$SDF[[var]]
  #deviance_resids <- gwr_model$SDF@data$deviance_resids
  response_resids <- gwr_model$SDF@data$Response_Residuals
  
  # Compute Standard Deviation of Deviance Residuals
  sd_resid <- sd(response_resids, na.rm = TRUE)
  
  df <- nrow(original.data) - length(street_measures) - 1  # Adjusting for predictors & intercept
  
  # Compute Standard Error (SE) for the local coefficient
  se <- sd_dev_resid / sqrt(nrow(original.data))

  # Compute pseudo-t statistic
  pseudo_t <- local_coef / se

  # Compute p-values
  p_values <- round(2 * (1 - pt(abs(pseudo_t), df)), 6)

  # Create temporary results data frame
  temp_df <- cbind(
    setNames(data.frame(local_coef), paste0("B_", var)), 
    setNames(data.frame(p_values), paste0("sig_", var))
  )
  
  # Combine with final results
  if (nrow(final_results_df) == 0) {
    final_results_df <- temp_df
  } else {
    final_results_df <- cbind(final_results_df, temp_df)
  }
}

original.data_final <- cbind(original.data, final_results_df)
View(original.data_final)
#st_write(original.data_final, "../GWR_NB_in_R.shp", delete_dsn = TRUE)

```

```{r GWR NB-based , echo = TRUE}

# Calculate Sum of Squared Deviance Residuals
SS_DevResid <- sum(deviance_resids^2)

# Calculate Total Sum of Squares (TSS)
y_mean <- mean(original.data$crime.count)
TSS <- sum((original.data$crime.count - y_mean)^2)

# Compute Pseudo R^2
pseudo_R2 <- 1 - (SS_DevResid / TSS)
pseudo_R2

# Number of observations
n <- nrow(original.data)

# Number of predictors (excluding intercept)
k <- length(gwr_model$SDF) - 2  # Subtracting intercept and sum.w

# Compute Adjusted Pseudo R^2
adj_pseudo_R2 <- 1 - ((1 - pseudo_R2) / (1 - (k / n)))
adj_pseudo_R2

```

